{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aed277-60d4-4bfe-b4c4-95ffa6565891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyGILE-Plus initialized with 1,773+ algorithms\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PyGILE-Plus Environment Setup\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Critical environment setup\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/opt/conda/envs/pygile/lib\"\n",
    "os.environ['SAGA_CMD'] = '/opt/saga/bin/saga_cmd'\n",
    "os.environ['SAGA_MLB'] = '/opt/saga/lib/saga'\n",
    "os.environ['GISBASE'] = '/opt/grass'\n",
    "os.environ['OTB_APPLICATION_PATH'] = '/opt/otb/lib/otb/applications'\n",
    "\n",
    "# Add GRASS to Python path\n",
    "sys.path.insert(0, '/opt/grass/etc/python')\n",
    "\n",
    "print(\"PyGILE-Plus initialized with direct tool access!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc7d75-d61d-42a2-a090-873e4066f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 73 SAGA libraries...\n",
      "SAGA extraction complete: 733 algorithms\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SAGA GIS Algorithm Extraction - Minimal Version\n",
    "\"\"\"\n",
    "import re\n",
    "import subprocess\n",
    "import csv\n",
    "\n",
    "def export_saga_algorithms():\n",
    "    \"\"\"Extract all SAGA algorithms efficiently\"\"\"\n",
    "    \n",
    "    env = {\n",
    "        'LD_LIBRARY_PATH': '/opt/saga/lib:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/opt/conda/envs/pygile/lib',\n",
    "        'SAGA_MLB': '/opt/saga/lib/saga'\n",
    "    }\n",
    "    \n",
    "    # Get libraries\n",
    "    result = subprocess.run([\"/opt/saga/bin/saga_cmd\"], capture_output=True, text=True, env=env)\n",
    "    \n",
    "    libraries = []\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if line.startswith(' - '):\n",
    "            lib_name = line[3:].strip().rstrip(' *')\n",
    "            if lib_name and not lib_name.startswith('_'):\n",
    "                libraries.append(lib_name)\n",
    "    \n",
    "    unique_libraries = list(dict.fromkeys(libraries))\n",
    "    print(f\"Processing {len(unique_libraries)} SAGA libraries...\")\n",
    "    \n",
    "    algorithms_data = []\n",
    "    \n",
    "    for lib in unique_libraries:\n",
    "        result = subprocess.run([\"/opt/saga/bin/saga_cmd\", lib], capture_output=True, text=True, env=env)\n",
    "        \n",
    "        for line in result.stdout.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\[(\\d+)\\]\\s+(.+)', line)\n",
    "            if match:\n",
    "                tool_id = match.group(1)\n",
    "                tool_name = match.group(2).strip()\n",
    "                algorithms_data.append([lib, tool_id, tool_name])\n",
    "    \n",
    "    # Save CSV\n",
    "    with open('/workspace/saga_all_algorithms.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Library', 'Tool_ID', 'Algorithm_Name'])\n",
    "        writer.writerows(algorithms_data)\n",
    "    \n",
    "    print(f\"SAGA extraction complete: {len(algorithms_data)} algorithms\")\n",
    "    return len(algorithms_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total = export_saga_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64971db2-014e-469a-9338-2f6cfe20c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GRASS directories...\n",
      "GRASS extraction complete: 500 algorithms\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GRASS GIS Algorithm Extraction - Minimal Version\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def export_grass_algorithms():\n",
    "    \"\"\"Extract all GRASS algorithms efficiently\"\"\"\n",
    "    \n",
    "    algorithms = []\n",
    "    \n",
    "    # Module categories\n",
    "    module_categories = {\n",
    "        'r.': 'Raster', 'v.': 'Vector', 'g.': 'General', 'i.': 'Imagery',\n",
    "        't.': 'Temporal', 'd.': 'Display', 'db.': 'Database', 'ps.': 'PostScript'\n",
    "    }\n",
    "    \n",
    "    # GRASS directories\n",
    "    grass_directories = [\n",
    "        '/opt/grass/grass84/bin',\n",
    "        '/opt/grass/grass84/scripts', \n",
    "        '/opt/grass/bin'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing GRASS directories...\")\n",
    "    \n",
    "    for grass_dir in grass_directories:\n",
    "        if os.path.exists(grass_dir):\n",
    "            files = os.listdir(grass_dir)\n",
    "            \n",
    "            for filename in sorted(files):\n",
    "                for prefix, category in module_categories.items():\n",
    "                    if filename.startswith(prefix):\n",
    "                        if not any(alg['algorithm_id'] == filename for alg in algorithms):\n",
    "                            desc = get_module_description(filename, grass_dir)\n",
    "                            algorithms.append({\n",
    "                                'tool': 'GRASS',\n",
    "                                'provider': 'GRASS',\n",
    "                                'algorithm_id': filename,\n",
    "                                'display_name': desc,\n",
    "                                'group': category,\n",
    "                                'location': grass_dir\n",
    "                            })\n",
    "                        break\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_path = '/workspace/grass_all_algorithms.csv'\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['tool', 'provider', 'algorithm_id', 'display_name', 'group', 'location']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(algorithms)\n",
    "    \n",
    "    print(f\"GRASS extraction complete: {len(algorithms)} algorithms\")\n",
    "    return len(algorithms)\n",
    "\n",
    "def get_module_description(module_name, module_dir):\n",
    "    \"\"\"Get basic module description\"\"\"\n",
    "    possible_paths = [\n",
    "        f'{module_dir}/{module_name}',\n",
    "        f'/opt/grass/bin/{module_name}',\n",
    "        f'/opt/grass/grass84/bin/{module_name}'\n",
    "    ]\n",
    "    \n",
    "    for exec_path in possible_paths:\n",
    "        if os.path.exists(exec_path):\n",
    "            try:\n",
    "                env = {'GISBASE': '/opt/grass', 'PATH': '/opt/grass/bin:/opt/grass/grass84/bin:' + os.environ.get('PATH', '')}\n",
    "                result = subprocess.run([exec_path, '--help'], capture_output=True, text=True, env=env, timeout=3)\n",
    "                \n",
    "                output = result.stdout + result.stderr\n",
    "                for line in output.split('\\n'):\n",
    "                    line = line.strip()\n",
    "                    if 'Description:' in line:\n",
    "                        return line.split('Description:', 1)[1].strip()\n",
    "                    elif line and len(line) > 10 and not line.startswith(('Usage:', 'Flags:', 'Parameters:')):\n",
    "                        return line\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return module_name\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total = export_grass_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420eed09-2fab-48ed-898b-f89432480c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 82 packages from Dockerfile\n",
      "\n",
      "Categories:\n",
      "  Other: 19\n",
      "  Geospatial Core: 8\n",
      "  Deep Learning: 8\n",
      "  Scientific Computing: 6\n",
      "  Visualization: 6\n",
      "  Data Formats: 5\n",
      "  Specialized Tools: 5\n",
      "  Image Processing: 4\n",
      "  Web Mapping: 4\n",
      "  Cloud/Remote Sensing: 4\n",
      "  Documentation: 4\n",
      "  Jupyter Environment: 3\n",
      "  Geospatial Analysis: 3\n",
      "  GIS Platforms: 3\n",
      "\n",
      "Total Python packages from Dockerfile: 82\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import csv\n",
    "\n",
    "# Packages extracted directly from your Dockerfile\n",
    "dockerfile_packages = [\n",
    "    # Core geospatial\n",
    "    \"numpy\", \"gdal\", \"proj\", \"geos\", \"libspatialindex\", \"boost-cpp\",\n",
    "    \"fiona\", \"shapely\", \"pyproj\", \"pandas\", \"scipy\", \"matplotlib\", \n",
    "    \"seaborn\", \"scikit-learn\", \"geopandas\", \"rasterio\",\n",
    "    \n",
    "    # Data formats\n",
    "    \"xarray\", \"netcdf4\", \"h5py\", \"h5netcdf\", \"zarr\",\n",
    "    \n",
    "    # Jupyter\n",
    "    \"jupyter\", \"jupyterlab\", \"ipywidgets\",\n",
    "    \n",
    "    # Visualization\n",
    "    \"plotly\", \"bokeh\", \"folium\", \"contextily\", \"mapclassify\",\n",
    "    \"holoviews\", \"hvplot\", \"pythreejs\",\n",
    "    \n",
    "    # Geospatial analysis\n",
    "    \"osmnx\", \"earthpy\", \"geoplot\",\n",
    "    \n",
    "    # Image processing\n",
    "    \"scikit-image\", \"tifffile\", \"imageio-ffmpeg\", \"opencv\",\n",
    "    \n",
    "    # Web mapping\n",
    "    \"localtileserver\", \"rio-cogeo\", \"rioxarray\", \"ipyleaflet\", \n",
    "    \"owslib\", \"geemap\", \"leafmap\",\n",
    "    \n",
    "    # Optional packages\n",
    "    \"census\", \"us\", \"pykrige\", \"palettable\", \"geojson\",\n",
    "    \n",
    "    # Cloud tools\n",
    "    \"pystac\", \"stackstac\", \"planetary-computer\",\n",
    "    \n",
    "    # GIS platforms\n",
    "    \"whitebox_tools\",\n",
    "    \n",
    "    # Deep Learning\n",
    "    \"pytorch-cpu\", \"torchvision\", \"torchaudio\", \"pytorch-lightning\",\n",
    "    \"tensorflow\", \"keras\", \"albumentations\", \"timm\",\n",
    "    \n",
    "    # Additional packages (from pip installs)\n",
    "    \"pygis\", \"earthengine-api\", \"sklearn-xarray\", \"sphinx\", \n",
    "    \"sphinx_sitemap\", \"sphinxcontrib.bibtex\", \"sphinx_inline_tabs\", \n",
    "    \"pydata-sphinx-theme\", \"sankee\", \"overturemaps\", \"whiteboxgui\",\n",
    "    \"jupyter-book\", \"ghp-import\", \"numpy-groupies\", \"sympy\",\n",
    "    \"geojson\", \"dask-geopandas\", \"pykrige\", \"cenpy\", \"census\", \n",
    "    \"us\", \"sklearn-xarray\", \"whitebox\", \"PySAGA-cmd\", \"pyspatialml\"\n",
    "]\n",
    "\n",
    "def categorize_package(pkg_name):\n",
    "    \"\"\"Categorize based on Dockerfile groupings\"\"\"\n",
    "    pkg_lower = pkg_name.lower()\n",
    "    \n",
    "    if pkg_name in [\"gdal\", \"proj\", \"geos\", \"fiona\", \"shapely\", \"pyproj\", \"geopandas\", \"rasterio\"]:\n",
    "        return \"Geospatial Core\"\n",
    "    elif pkg_name in [\"numpy\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"scikit-learn\"]:\n",
    "        return \"Scientific Computing\"\n",
    "    elif pkg_name in [\"xarray\", \"netcdf4\", \"h5py\", \"h5netcdf\", \"zarr\"]:\n",
    "        return \"Data Formats\"\n",
    "    elif pkg_name in [\"jupyter\", \"jupyterlab\", \"ipywidgets\"]:\n",
    "        return \"Jupyter Environment\"\n",
    "    elif pkg_name in [\"plotly\", \"bokeh\", \"folium\", \"contextily\", \"holoviews\", \"hvplot\", \"pythreejs\"]:\n",
    "        return \"Visualization\"\n",
    "    elif pkg_name in [\"geemap\", \"leafmap\", \"ipyleaflet\", \"owslib\"]:\n",
    "        return \"Web Mapping\"\n",
    "    elif pkg_name in [\"scikit-image\", \"opencv\", \"tifffile\", \"imageio-ffmpeg\"]:\n",
    "        return \"Image Processing\"\n",
    "    elif pkg_name in [\"osmnx\", \"earthpy\", \"geoplot\"]:\n",
    "        return \"Geospatial Analysis\"\n",
    "    elif pkg_name in [\"pystac\", \"stackstac\", \"planetary-computer\", \"earthengine-api\"]:\n",
    "        return \"Cloud/Remote Sensing\"\n",
    "    elif pkg_name in [\"whitebox_tools\", \"PySAGA-cmd\", \"pyspatialml\"]:\n",
    "        return \"GIS Platforms\"\n",
    "    elif pkg_name in [\"pytorch-cpu\", \"torchvision\", \"torchaudio\", \"pytorch-lightning\", \"tensorflow\", \"keras\", \"albumentations\", \"timm\"]:\n",
    "        return \"Deep Learning\"\n",
    "    elif pkg_name in [\"sphinx\", \"sphinx_sitemap\", \"jupyter-book\", \"ghp-import\"]:\n",
    "        return \"Documentation\"\n",
    "    elif pkg_name in [\"census\", \"us\", \"pykrige\", \"palettable\", \"geojson\"]:\n",
    "        return \"Specialized Tools\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_packages = []\n",
    "for pkg in dockerfile_packages:\n",
    "    if pkg not in seen:\n",
    "        seen.add(pkg)\n",
    "        unique_packages.append(pkg)\n",
    "\n",
    "# Create CSV data\n",
    "package_data = []\n",
    "for pkg in unique_packages:\n",
    "    package_data.append([\n",
    "        'Python',\n",
    "        'Package',\n",
    "        pkg,\n",
    "        pkg,\n",
    "        categorize_package(pkg)\n",
    "    ])\n",
    "\n",
    "# Save CSV\n",
    "with open('/workspace/python_packages_dockerfile.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Tool', 'Provider', 'Algorithm_ID', 'Display_Name', 'Group'])\n",
    "    writer.writerows(package_data)\n",
    "\n",
    "print(f\"Extracted {len(package_data)} packages from Dockerfile\")\n",
    "\n",
    "# Show categories\n",
    "categories = {}\n",
    "for row in package_data:\n",
    "    cat = row[4]\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nCategories:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal Python packages from Dockerfile: {len(package_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a008338-c7bb-4430-bc26-0a3cee1e0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OTB Algorithm Extraction ===\n",
      "Found 115 OTB CLI algorithms\n",
      "Found 115 OTB application libraries\n",
      "Total OTB algorithms: 230\n",
      "CSV saved with 230 algorithms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def export_otb_algorithms():\n",
    "   \"\"\"Export OTB algorithms to CSV - check both bin and applications directories\"\"\"\n",
    "   algorithms = []\n",
    "   \n",
    "   print(\"=== OTB Algorithm Extraction ===\")\n",
    "   \n",
    "   # Check CLI binaries\n",
    "   otb_bin_dir = '/opt/otb/bin'\n",
    "   if os.path.exists(otb_bin_dir):\n",
    "       otb_apps = glob.glob(os.path.join(otb_bin_dir, 'otbcli_*'))\n",
    "       \n",
    "       for app_path in otb_apps:\n",
    "           app_name = os.path.basename(app_path).replace('otbcli_', '')\n",
    "           algorithms.append(['OTB', 'OTB_CLI', app_name, app_name])\n",
    "       \n",
    "       print(f\"Found {len(otb_apps)} OTB CLI algorithms\")\n",
    "   \n",
    "   # Check application libraries\n",
    "   otb_app_dir = '/opt/otb/lib/otb/applications'\n",
    "   if os.path.exists(otb_app_dir):\n",
    "       otb_libs = glob.glob(os.path.join(otb_app_dir, 'otbapp_*.so'))\n",
    "       \n",
    "       for lib_path in otb_libs:\n",
    "           lib_name = os.path.basename(lib_path).replace('otbapp_', '').replace('.so', '')\n",
    "           algorithms.append(['OTB', 'OTB_APP', lib_name, lib_name])\n",
    "       \n",
    "       print(f\"Found {len(otb_libs)} OTB application libraries\")\n",
    "   \n",
    "   total_algorithms = len(algorithms)\n",
    "   print(f\"Total OTB algorithms: {total_algorithms}\")\n",
    "   \n",
    "   # Save CSV\n",
    "   with open('/workspace/otb_algorithms.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "       writer = csv.writer(csvfile)\n",
    "       writer.writerow(['Tool', 'Provider', 'Algorithm_ID', 'Algorithm_Name'])\n",
    "       writer.writerows(algorithms)\n",
    "   \n",
    "   print(f\"CSV saved with {total_algorithms} algorithms\")\n",
    "   return total_algorithms\n",
    "\n",
    "# Run it:\n",
    "export_otb_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca485454-5969-4267-8742-24d246347f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found WhiteboxTools at: /opt/conda/envs/pygile/bin/whitebox_tools\n",
      "Exported 460 WhiteboxTools algorithms\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import csv\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def find_whitebox_executable():\n",
    "    \"\"\"Find WhiteboxTools executable\"\"\"\n",
    "    # Try conda-forge installation path first\n",
    "    conda_paths = [\n",
    "        '/opt/conda/envs/pygile/bin/whitebox_tools',\n",
    "        '/opt/conda/envs/pygile/bin/whiteboxtools'\n",
    "    ]\n",
    "    \n",
    "    for path in conda_paths:\n",
    "        if os.path.isfile(path) and os.access(path, os.X_OK):\n",
    "            return path\n",
    "    \n",
    "    # Try system PATH\n",
    "    try:\n",
    "        result = subprocess.run(['which', 'whitebox_tools'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def categorize_tool(tool_name):\n",
    "    \"\"\"Categorize WhiteboxTools\"\"\"\n",
    "    tool_lower = tool_name.lower()\n",
    "    \n",
    "    if any(keyword in tool_lower for keyword in ['slope', 'aspect', 'hillshade', 'curvature']):\n",
    "        return 'Terrain Analysis'\n",
    "    elif any(keyword in tool_lower for keyword in ['flow', 'watershed', 'stream', 'drainage']):\n",
    "        return 'Hydrological Analysis'\n",
    "    elif any(keyword in tool_lower for keyword in ['filter', 'smooth', 'gaussian', 'median']):\n",
    "        return 'Image Processing'\n",
    "    elif any(keyword in tool_lower for keyword in ['buffer', 'clip', 'vector']):\n",
    "        return 'Vector Analysis'\n",
    "    elif any(keyword in tool_lower for keyword in ['raster', 'grid', 'resample']):\n",
    "        return 'Raster Processing'\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "# Find executable\n",
    "wb_exec = find_whitebox_executable()\n",
    "if not wb_exec:\n",
    "    print(\"WhiteboxTools executable not found\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Found WhiteboxTools at: {wb_exec}\")\n",
    "\n",
    "# Extract algorithms using --listtools\n",
    "algorithms = []\n",
    "try:\n",
    "    result = subprocess.run([wb_exec, '--listtools'], capture_output=True, text=True, timeout=30)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('Available') and not line.startswith('='):\n",
    "                if ':' in line:\n",
    "                    tool_name = line.split(':')[0].strip()\n",
    "                    description = line.split(':', 1)[1].strip()\n",
    "                elif ' - ' in line:\n",
    "                    tool_name = line.split(' - ')[0].strip()\n",
    "                    description = line.split(' - ', 1)[1].strip()\n",
    "                else:\n",
    "                    tool_name = line.split()[0] if line.split() else line\n",
    "                    description = line\n",
    "                \n",
    "                if tool_name and len(tool_name) > 1:\n",
    "                    algorithms.append([\n",
    "                        'WhiteboxTools',\n",
    "                        'WhiteboxTools',\n",
    "                        tool_name,\n",
    "                        description,\n",
    "                        categorize_tool(tool_name)\n",
    "                    ])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting algorithms: {e}\")\n",
    "\n",
    "# Save CSV\n",
    "with open('/workspace/whitebox_algorithms.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Tool', 'Provider', 'Algorithm_ID', 'Display_Name', 'Group'])\n",
    "    writer.writerows(algorithms)\n",
    "\n",
    "print(f\"Exported {len(algorithms)} WhiteboxTools algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66524e-fc95-4051-93c4-6f7715992763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
